{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report for the WeRateDogs Project\n",
    "\n",
    "### By Faith Ebosetale Omohodion\n",
    "\n",
    "Date: 26th October 2022.\n",
    "\n",
    "The wrangling WeRateDogs project was a huge climb to take in my data analytic career as it required me to push myself more than I could imagine, but every effort and work put into it was worth it as my knowledge was broaden and more advance in problem solving compare to before I took the project. \n",
    "\n",
    "Firstly, I had to gain access to the twitter developers handle so I can query the data, I followed the instructions and applied for approval which was granted and I got my unique keys and using the Tweepy library (an access library for the twitter API) with that I was able to query the WeRateDogs tweet data  via the Twitter API (tweet_json.txt). Then I had to programmatically download the image_predictions tsv file from the url which was provided. The requests library came in handy for this very purpose. I was able to access all the file content by using the get function on the url and then write that content to an empty file. Thus, just like that I had two files now. And on using the library's get_status function on each and every tweet id, I was able to get each tweet's content. I converted this content to the json format first for future use and then wrote it to a text file called tweet_json. Doing all this for every tweet required me to use a 'for' loop. Every tweet's json object in the text file was then read one by one into an empty list. The sole purpose for that was to access all the json objects to extract the favourite and retweet counts for each and every tweet. After extracting all the counts I used them and their corresponding tweet ids to create a pandas dataframe.\n",
    "\n",
    "Finally I had 3 datasets gathered in total which I had to assess for quality and tidiness issues and clean. Undoubtedly the twitter archives table had most of the issues. This was because it was the largest of the 3 tables and had a lot of features regarding the tweets like tweet text, date and time of the tweet, etc. A lot of the issues could just be spotted by the naked eye thanks to pandas functions like head, tail and sample. Others required a little more analysis, mainly through summaries or filtering out certain sections of the data and evaluating the features. The info and value_counts functions were frequently used for the same. Most of the tidiness issues involved joining of the tables and melting certain features into a single column.\n",
    "\n",
    "The final part was the most code oriented part which was cleaning. For each operation, I used the define, code and test format wherein I first defined the operation I wanted to perform, then wrote the code for it and finally conducted tests on the dataset to see if that particular operation was performed. Various pandas functions were used, some in isolation and some together. A lot of the operations involved extracting data from certain features and replacing the inaccurate or incomplete column data with that data. A lot of unwanted rows and columns especially the ones with retweets were removed.\n",
    "\n",
    "In conclusion this project got me to work with problems that come with creating your own datasets, finding out the various quality and tidiness related issues, cleaning up those issues to make the datasets analysis worthy and finally drawing analytical decisions and visualizations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
